# General parameters
seed: 42
max_sent_size: 32
min_freq: 5 # for building the vocab

train:
  batch_size: 32
  num_workers: 8
  max_epochs: 30
  max_steps: -1
  learning_rate: 1.e-4
  min_learning_rate: 1.e-5
  label_smoothing: 0
  weight_decay: 0


model_resume: './src/models/transformer/logs/aug_and_lr_scheduler_baseline_model/epoch=14-step=193920.ckpt'
pretrain_path: './src/models/transformer/logs/aug_and_lr_scheduler_baseline_model/epoch=14-step=193920.ckpt'
logs_dir: './src/models/transformer/logs'
default_root_dir: './src/models/transformer/'

model:
  model_name: 'aug_and_lr_scheduler_baseline_model'
  input_dim: 3 # input channels
  embed_dim: 512
  hidden_dim: 512
  output_dim: 
  num_heads: 8
  num_layers: 4
  ff_expantion: 4  # 512 -> 1024
  encoder_dropout: 0.5 # dropout of resnet linear layer
  decoder_dropout: 0.1
  max_length: 32

datasets:
  train_dataset: '/datasets/coco/train2014/'
  train_caption: '/datasets/coco/annotations/captions_train2014.json'
  val_dataset: '/datasets/coco/val2014/'
  val_caption: '/datasets/coco/annotations/captions_val2014.json'
